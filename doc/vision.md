# Техническое видение проекта: LLM-ассистент для подбора аниме

## 1. Технологии

### Основные технологии
- **Язык программирования:** Python 3.11+
- **Telegram Bot API:** aiogram 3.x (современная асинхронная библиотека)  
- **LLM API:** OpenRouter (доступ к различным моделям через единый API)
- **База данных:** Не используем (для MVP достаточно хранения в памяти/файлах)
- **Управление зависимостями:** uv (современный менеджер пакетов Python)
- **Виртуальное окружение:** uv venv (встроенное в uv)

### Дополнительные инструменты
- **Веб-сервер:** Не нужен (polling вместо webhook)
- **Контейнеризация:** Docker (для деплоя)
- **Хранение данных:** JSON-файлы для простых настроек и кэша
- **Кэширование:** Локальные файлы для сохранения ответов LLM
- **История диалогов:** Временное хранение в памяти с возможностью сохранения сессий

### Основные зависимости
```toml
# pyproject.toml
[project]
dependencies = [
    "aiogram~=3.0",
    "openai",
    "python-dotenv",
    "aiofiles",
]
```

## 2. Принципы разработки

### Основные принципы
- **KISS (Keep It Simple, Stupid)** - максимальная простота решений
- **YAGNI (You Aren't Gonna Need It)** - не реализуем функции "на будущее"
- **MVP-подход** - минимально жизнеспособный продукт
- **Fail Fast** - быстрое обнаружение и исправление ошибок

### Подходы к разработке
- **Итеративная разработка** - небольшие рабочие версии
- **Функциональное программирование** (где возможно) для простоты тестирования
- **Асинхронное программирование** для работы с Telegram API и LLM
- **Конфигурация через переменные окружения** - отделение настроек от кода

### Качество кода
- **Простая архитектура** - минимум абстракций
- **Читаемый код** - понятные имена переменных и функций
- **Комментарии в коде** - объяснение сложной логики
- **Обработка ошибок** - graceful degradation при сбоях LLM/API

### Процесс разработки
- **Тестирование:** Ручное тестирование для MVP
- **Версионирование:** Git Flow (feature branches, develop, main)
- **Документация:** Комментарии в коде, без docstrings

## 3. Структура проекта

```
llmstartj/
├── src/                    # Исходный код
│   ├── bot.py             # Главный файл бота (точка входа)
│   ├── handlers/          # Обработчики команд и сообщений
│   │   ├── __init__.py
│   │   ├── start.py       # /start, /help команды
│   │   └── anime.py       # Логика подбора аниме
│   ├── services/          # Бизнес-логика
│   │   ├── __init__.py
│   │   ├── llm_service.py # Работа с OpenRouter/LLM
│   │   └── cache_service.py # Кэширование ответов
│   └── utils/             # Утилиты
│       ├── __init__.py
│       ├── config.py      # Конфигурация
│       ├── logger.py      # Настройка логирования
│       └── prompts.py     # Промпты для LLM
├── data/                  # Данные (кэш, настройки)
│   └── cache/            # Кэш LLM ответов
├── tests/                # Тесты (для будущего расширения)
├── Dockerfile            # Docker конфигурация в корне
├── .env.example          # Пример переменных окружения
├── .gitignore
├── pyproject.toml        # Зависимости и конфигурация проекта (uv)
├── uv.lock              # Лок-файл зависимостей (uv)
├── .python-version      # Версия Python для uv
└── README.md            # Документация запуска
```

## 4. Архитектура проекта

### Архитектурный подход
- **Layered Architecture** (слоеная архитектура) - простая и понятная
- **Event-driven** - реакция на сообщения от пользователей
- **Асинхронная обработка** - неблокирующие операции
- **Stateless** - состояния диалога в памяти (простые флаги)

### Основные компоненты

```
┌─────────────────┐
│   Telegram API  │ ← Входящие сообщения
└─────────────────┘
         │
┌─────────────────┐
│    Handlers     │ ← Маршрутизация команд/сообщений
│  (/start, text) │
└─────────────────┘
         │
┌─────────────────┐
│    Services     │ ← Бизнес-логика
│   (LLM, Cache)  │
└─────────────────┘
         │
┌─────────────────┐
│  External APIs  │ ← OpenRouter, файловая система
│ (OpenRouter, FS)│
└─────────────────┘
```

### Поток данных
1. **Пользователь** отправляет сообщение в Telegram
2. **Handler** определяет тип сообщения (команда/текст)
3. **Service** обрабатывает запрос (вызов LLM, кэширование)
4. **Ответ** отправляется обратно пользователю

### Особенности реализации
- **Без middleware** - простая обработка сообщений
- **Состояния в памяти** - dict для хранения текущего диалога пользователя
- **Синхронная обработка** - без очередей, прямая обработка запросов

## 5. Модель данных

### Основные сущности

**1. Пользовательская сессия (в памяти):**
```python
{
    user_id: {
        "state": "waiting_preferences",  # состояние диалога
        "preferences": {},               # предпочтения пользователя
        "last_query": "",               # последний запрос
        "conversation_history": []       # история сообщений (без ограничений)
    }
}
```

**2. Кэш LLM ответов (JSON файлы):**
```python
{
    "query_hash": {
        "query": "подбери аниме про школу",
        "response": "Рекомендую...",
        "timestamp": 1699123456,
        "model": "gpt-4"
    }
}
```

**3. Конфигурация (переменные окружения):**
- `TELEGRAM_BOT_TOKEN` - токен Telegram бота
- `OPENROUTER_API_KEY` - ключ API OpenRouter
- `OPENROUTER_MODEL` - модель LLM (по умолчанию)
- `CACHE_TTL_HOURS` - время жизни кэша в часах

### Особенности хранения
- **Без персистентной БД** - все в памяти и файлах
- **Без валидации данных** - доверяем Telegram API
- **Статистика не ведется** - фокус на MVP функционал
- **История диалога не ограничена** - простота реализации

## 6. Работа с LLM

### Интеграция с LLM
- **Провайдер:** OpenRouter (доступ к разным моделям)
- **Основная модель:** Meta: Llama 3.1 8B Instruct (free) - оптимальная бесплатная модель
- **Контекст:** 4k токенов (достаточно для диалогов о подборе аниме)

### Промпт-инжиниринг
```python
SYSTEM_PROMPT = """
Ты - Сайтама из аниме "Ванпанчмен", который теперь помогает подбирать аниме.
Говори в его стиле: просто, немного равнодушно, но со знанием дела.
Используй фразы типа "Ладно...", "Хм...", "Окей...", "Не самое интересное, но...".
Задавай прямые вопросы о предпочтениях, давай честные рекомендации.
Отвечай на русском языке в характере Сайтамы.
"""
```

### Стратегия кэширования
- **Хэширование запросов:** MD5 от нормализованного текста
- **TTL кэша:** 24 часа (настраиваемо через переменные окружения)
- **Размер кэша:** без ограничений для MVP
- **Формат хранения:** JSON файлы в папке `data/cache/`

### Обработка ошибок
- **Retry с exponential backoff** при временных сбоях API
- **Graceful degradation:** "Извините, сервис временно недоступен" при полном отказе
- **Логирование ошибок** для последующего анализа

### Особенности работы
- **Без модерации контента** - доверяем LLM и фильтрам OpenRouter
- **Без rate limiting** - простота для MVP
- **Асинхронные вызовы** - неблокирующая обработка запросов

## 7. Мониторинг

### Минимальный мониторинг для MVP
- **Логирование запросов к LLM:** сохранение всех отправляемых промптов
- **Логирование ответов LLM:** сохранение полученных ответов
- **Формат логов:** простой текстовый формат с timestamp
- **Инструмент:** Python logging (встроенная библиотека)

### Структура логов
```
[2024-01-15 10:30:45] LLM_REQUEST: user_id=12345, query="подбери аниме про школу"
[2024-01-15 10:30:47] LLM_RESPONSE: user_id=12345, response="Рекомендую..."
```

### Особенности
- **Логи в stdout** - для простоты в Docker
- **Без дашбордов** - фокус на MVP
- **Без сложных метрик** - только запросы/ответы LLM

## 8. Сценарии работы

### Стиль общения
- **Персонаж:** Сайтама из "Ванпанчмен" - расслабленный, немного равнодушный, но помогающий
- **Манера речи:** Простые фразы, иногда скучающий тон, но знающий об аниме

### Основные сценарии

**1. Первое знакомство:**
```
Пользователь: /start
Бот: Ох... Привет. Я тут подбираю аниме. Не самая захватывающая работа, но ладно. 
     Расскажи, что хочешь посмотреть, и я что-нибудь найду.
```

**2. Быстрый подбор:**
```
Пользователь: Посоветуй что-то интересное
Бот: Хм, "интересное"... Это слишком размыто. Ты любишь драки? Романтику? 
     Или может комедии? Дай хоть какую-то зацепку.
```

**3. Детальный подбор:**
```
Пользователь: Хочу аниме про школу
Бот: Школьное аниме, значит... Таких тысячи. Тебе романтику с персиками на щеках 
     или может что-то с суперспособностями? А то я заскучаю.
```

**4. Рекомендация:**
```
Бот: Ладно, вот тебе "Моя геройская академия". Про школу героев, драки есть, 
     персонажи не раздражают. Серий много, но смотрится нормально.
```

### Дополнительные команды
- `/help` - "Чего объяснять... Пиши что хочешь посмотреть, я найду."
- `/reset` - "Окей, забыл что ты хотел. Начнем заново."

### Готовые категории
- `/top` - "Ну ладно, вот популярные аниме. Хотя не понимаю, зачем всем одно и то же..."
- `/new` - "Новинки? Окей, посмотрим что там наделали в этом сезоне..."
- `/classic` - "Классика... Хм, вот что действительно стоящее из старого."

### Пагинация ответов
- **Длинные списки** разбиваются на страницы по 3-5 рекомендаций
- **Inline кнопки:** "Следующая страница", "Предыдущая страница"
- **Стиль Сайтамы:** "Еще есть варианты, если хочешь..."

### Особенности стиля Сайтамы
- **Простота речи** - короткие, понятные фразы
- **Легкая скука** - "Ладно...", "Хм...", "Окей..."
- **Знание дела** - несмотря на равнодушие, дает хорошие советы
- **Прямолинейность** - говорит как есть, без приукрашивания
- **Без оценок** - не просит feedback, просто дает рекомендации

## 9. Деплой

### Стратегия деплоя
- **Контейнеризация:** Docker для изоляции и переносимости
- **Простой деплой:** Один контейнер на платформе
- **Без оркестрации:** Не нужен Kubernetes для MVP

### Docker настройка
```dockerfile
FROM python:3.11-slim

# Установка uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /app

# Копирование файлов конфигурации
COPY pyproject.toml uv.lock ./

# Установка зависимостей через uv
RUN uv sync --frozen --no-cache

# Копирование исходного кода
COPY src/ ./src/
COPY data/ ./data/

ENV PYTHONPATH=/app
CMD ["uv", "run", "src/bot.py"]
```

### Варианты хостинга
- **Продакшен:** Railway, Render - простота деплоя из Git
- **Разработка:** Локальный запуск через Docker или Python
- **Автодеплой:** Связка с GitHub для автоматического деплоя

### CI/CD процесс
```yaml
# .github/workflows/deploy.yml
on:
  push:
    branches: [main]
steps:
  - Checkout code
  - Build Docker image  
  - Deploy to Railway/Render
  - Health check
```

### Скрипты управления
```bash
# scripts/start.sh - запуск локально через uv
# scripts/build.sh - сборка Docker образа
# scripts/deploy.sh - деплой на платформу
```

### Процесс деплоя
1. **Push в main branch** → автоматический деплой
2. **Локальная разработка:** `uv run src/bot.py` или `docker-compose up`
3. **Установка зависимостей:** `uv sync` для синхронизации
4. **Переменные окружения** настраиваются в панели платформы
5. **Health check** через логи платформы

## 10. Подход к конфигурированию

### Переменные окружения
```bash
# Обязательные
TELEGRAM_BOT_TOKEN=your_bot_token_here
OPENROUTER_API_KEY=your_openrouter_key_here

# Опциональные с дефолтами
OPENROUTER_MODEL=meta-llama/llama-3.1-8b-instruct:free
CACHE_TTL_HOURS=24
LOG_LEVEL=INFO
```

### Конфигурационная система
- **Python-dotenv** для локальной разработки (.env файл)
- **Переменные окружения** для продакшена (Railway/Render)
- **Простая валидация** - проверка наличия обязательных переменных
- **Дефолтные значения** для опциональных настроек в коде

### Файл .env.example
```bash
TELEGRAM_BOT_TOKEN=1234567890:AABBCCDDeeFFggHHiiJJkkLLmmNNooP
OPENROUTER_API_KEY=sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxx
OPENROUTER_MODEL=meta-llama/llama-3.1-8b-instruct:free
CACHE_TTL_HOURS=24
LOG_LEVEL=INFO
```

### Особенности
- **Только переменные окружения** - без JSON/YAML файлов
- **Единая конфигурация** для всех сред (dev/prod)
- **Без сложной валидации** API ключей - доверяем провайдерам
- **Минимализм** - только необходимые настройки

## 11. Подход к логгированию

### Система логирования
- **Python logging** - встроенная библиотека
- **Структурированные логи** - timestamp, level, message, user_id
- **Вывод в stdout** - для Docker и облачных платформ

### Уровни логирования
- **INFO** - запросы/ответы LLM, команды пользователей, полные тексты сообщений
- **ERROR** - ошибки API, исключения
- **DEBUG** - детальная информация (только для разработки)

### Формат логов
```
[2024-01-15 10:30:45] INFO - User 12345 sent command: /start
[2024-01-15 10:30:46] INFO - User 12345 message: "подбери аниме про школу"
[2024-01-15 10:30:47] INFO - LLM request for user 12345: "подбери аниме про школу"
[2024-01-15 10:30:49] INFO - LLM response for user 12345: "Ладно, вот..."
[2024-01-15 10:31:00] ERROR - OpenRouter API error: Rate limit exceeded
```

### Настройка логирования
- **Конфигурация:** через переменную `LOG_LEVEL=INFO` (по умолчанию)
- **Без ротации логов** - облачная платформа управляет
- **Логирование полных сообщений** - для анализа и отладки
- **Простота настройки** - минимальная конфигурация

### Особенности
- **Все в stdout** - совместимость с Docker и облачными платформами
- **Включены пользовательские сообщения** - полная картина диалогов
- **Минимальная настройка** - фокус на простоте для MVP

---

## Заключение

Техническое видение создано с фокусом на принцип KISS - максимальная простота для MVP. 
Проект использует проверенные технологии, минимальную архитектуру и готов к быстрому развертыванию.
Бот Сайтама поможет пользователям находить аниме в своем уникальном стиле!

